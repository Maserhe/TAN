{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a23340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftWare\\Anaconda\\envs\\fa_clip\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "\n",
    "import json\n",
    "import multiprocessing\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utils import base_path, ShoesDataset, FashionIQDataset, targetpad_transform, CIRRDataset,squarepad_transform\n",
    "from utils import collate_fn, update_train_running_results, set_train_bar_description, save_model, \\\n",
    "    extract_index_features, generate_randomized_fiq_caption, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b5eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_list = []\n",
    "\n",
    "lines_train = [] # 用于保存每一行字符串的数组\n",
    "lines_test = [] # 用于保存每一行字符串的数组\n",
    "\n",
    "base_path = \"E://Code//data//shoesDataset//\"\n",
    "\n",
    "\n",
    "file_path_train = base_path + \"data//shoes//shoes-cap-train.txt\"\n",
    "file_path_test = base_path + \"data//shoes//shoes-cap-test.txt\"\n",
    "\n",
    "with open(file_path_test, 'r') as file:\n",
    "    for line in file:\n",
    "        temp_list = line.strip().split(';')\n",
    "        lines_test.append(temp_list)  # strip()用于移除行末尾的换行符和空白字符\n",
    "        image_path_list.append(temp_list[0])\n",
    "        image_path_list.append(temp_list[1])\n",
    "\n",
    "with open(file_path_train, 'r') as file:\n",
    "    for line in file:\n",
    "        temp_list = line.strip().split(';')\n",
    "        lines_train.append(temp_list)  # strip()用于移除行末尾的换行符和空白字符\n",
    "        image_path_list.append(temp_list[0])\n",
    "        image_path_list.append(temp_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de72a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42892"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c814b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17954, 3492)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines_train),len(lines_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be7021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_list = list(set(image_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4271cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10776"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72efa131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E://Code//data//shoesDataset//data/shoes/attributedata/womens_rain_boots/0/img_womens_rain_boots_269.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path + image_path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dacf7d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftWare\\Anaconda\\envs\\fa_clip\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocess = targetpad_transform(1.25, 288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee65049",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = preprocess(PIL.Image.open(base_path + image_path_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5264d0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 288, 288])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ecfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a9e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoes val dataset in classic mode initialized\n",
      "Shoes test dataset in relative mode initialized\n",
      "Shoes train dataset in relative mode initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftWare\\Anaconda\\envs\\fa_clip\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocess = targetpad_transform(1.25, 288)\n",
    "classic_val_dataset = ShoesDataset('val', 'classic', preprocess)\n",
    "relative_val_dataset = ShoesDataset('test', 'relative', preprocess)\n",
    "relative_train_dataset = ShoesDataset('train', 'relative', preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de5f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10776, 3492, 17954)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classic_val_dataset.__len__(), relative_val_dataset.__len__(), relative_train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e398e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_val_loader = DataLoader(dataset=classic_val_dataset, batch_size=32, num_workers=4,\n",
    "                                pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "relative_val_loader = DataLoader(dataset=relative_val_dataset, batch_size=32, num_workers=4,\n",
    "                                pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "relative_train_loader = DataLoader(dataset=relative_train_dataset, batch_size=32, num_workers=4,\n",
    "                                pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea935927",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (image_name_A, image_A) = next(enumerate(classic_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b54f757d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,\n",
       " 'data/shoes/attributedata/womens_stiletto/1/img_womens_stiletto_1580.jpg',\n",
       " torch.Size([32, 3, 288, 288]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_name_A), image_name_A[0], image_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ccad778",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (reference_name_B, target_name_B, image_captions_B)  = next(enumerate(relative_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc742f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, torch.Size([32, 3, 288, 288]), torch.Size([32, 3, 288, 288]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_captions_B),reference_name_B.shape, target_name_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de983d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (reference_image_C, target_image_C, image_captions_C)  = next(enumerate(relative_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8dd6e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 288, 288]), torch.Size([32, 3, 288, 288]), 32, 32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_image_C.shape, target_image_C.shape, len(image_captions_C), len(image_captions_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fff5945a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are teal suede with closed toes',\n",
       " 'are suede, with a buckle',\n",
       " 'are shiny maroon with no straps',\n",
       " 'have clear plastic uppers',\n",
       " 'are brown leather sandals',\n",
       " 'are essentially identical',\n",
       " 'have straps on the side',\n",
       " 'have laces and brown accents',\n",
       " 'are a tennis shoe with two velcro closures',\n",
       " 'is shiny blue, not black',\n",
       " 'has a squared toe',\n",
       " 'are pink and white, with no laces',\n",
       " 'are silver, not white',\n",
       " 'has a wider heel and is brown',\n",
       " 'have a strap and a thicker heel',\n",
       " 'have two straps',\n",
       " 'are slip on shoes',\n",
       " 'are silver',\n",
       " 'are gold, not silver',\n",
       " 'are matte, with a closed toe',\n",
       " 'are black with gold embellishments',\n",
       " 'are black with a white paisley print',\n",
       " 'are black with some red',\n",
       " 'is black with laces',\n",
       " 'are solid black',\n",
       " 'has a thinner heel and straps',\n",
       " 'are pale metallic, not black',\n",
       " 'is black with a thinner, higher heel',\n",
       " 'has a taller heel and thicker buckles',\n",
       " 'is black, with no thick buckles',\n",
       " 'are lighter brown with a higher heel',\n",
       " 'has a higher platform, and is black and purple']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_captions_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17777824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fa_clip] *",
   "language": "python",
   "name": "conda-env-fa_clip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
