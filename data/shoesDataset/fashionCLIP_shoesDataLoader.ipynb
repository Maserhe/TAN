{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48582c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionIQDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FashionIQ dataset class which manage FashionIQ data.\n",
    "    The dataset can be used in 'relative' or 'classic' mode:\n",
    "        - In 'classic' mode the dataset yield tuples made of (image_name, image)\n",
    "        - In 'relative' mode the dataset yield tuples made of:\n",
    "            - (reference_image, target_image, image_captions) when split == train\n",
    "            - (reference_name, target_name, image_captions) when split == val\n",
    "            - (reference_name, reference_image, image_captions) when split == test\n",
    "    The dataset manage an arbitrary numbers of FashionIQ category, e.g. only dress, dress+toptee+shirt, dress+shirt...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split: str, dress_types: List[str], mode: str, preprocess: callable):\n",
    "        \"\"\"\n",
    "        :param split: dataset split, should be in ['test', 'train', 'val']\n",
    "        :param dress_types: list of fashionIQ category\n",
    "        :param mode: dataset mode, should be in ['relative', 'classic']:\n",
    "            - In 'classic' mode the dataset yield tuples made of (image_name, image)\n",
    "            - In 'relative' mode the dataset yield tuples made of:\n",
    "                - (reference_image, target_image, image_captions) when split == train\n",
    "                - (reference_name, target_name, image_captions) when split == val\n",
    "                - (reference_name, reference_image, image_captions) when split == test\n",
    "        :param preprocess: function which preprocesses the image\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.dress_types = dress_types\n",
    "        self.split = split\n",
    "\n",
    "        if mode not in ['relative', 'classic']:\n",
    "            raise ValueError(\"mode should be in ['relative', 'classic']\")\n",
    "        if split not in ['test', 'train', 'val']:\n",
    "            raise ValueError(\"split should be in ['test', 'train', 'val']\")\n",
    "        for dress_type in dress_types:\n",
    "            if dress_type not in ['dress', 'shirt', 'toptee']:\n",
    "                raise ValueError(\"dress_type should be in ['dress', 'shirt', 'toptee']\")\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        # get triplets made by (reference_image, target_image, a pair of relative captions)\n",
    "        self.triplets: List[dict] = []\n",
    "        for dress_type in dress_types:\n",
    "            with open(base_path / 'fashionIQ_dataset' / 'captions' / f'cap.{dress_type}.{split}.json') as f:\n",
    "                self.triplets.extend(json.load(f))\n",
    "\n",
    "        # get the image names\n",
    "        self.image_names: list = []\n",
    "        for dress_type in dress_types:\n",
    "            with open(base_path / 'fashionIQ_dataset' / 'image_splits' / f'split.{dress_type}.{split}.json') as f:\n",
    "                self.image_names.extend(json.load(f))\n",
    "\n",
    "        print(f\"FashionIQ {split} - {dress_types} dataset in {mode} mode initialized\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            if self.mode == 'relative':\n",
    "                image_captions = self.triplets[index]['captions']\n",
    "                reference_name = self.triplets[index]['candidate']\n",
    "\n",
    "                if self.split == 'train':\n",
    "                    reference_image_path = base_path / 'fashionIQ_dataset' / 'images' / f\"{reference_name}.jpg\"\n",
    "                    # reference_image = self.preprocess(PIL.Image.open(reference_image_path))\n",
    "                    reference_image = self.preprocess(images = PIL.Image.open(reference_image_path), return_tensors='pt', padding=True)\n",
    "                    reference_image['pixel_values'] = reference_image['pixel_values'].squeeze(0)\n",
    "                    \n",
    "                    target_name = self.triplets[index]['target']\n",
    "                    target_image_path = base_path / 'fashionIQ_dataset' / 'images' / f\"{target_name}.jpg\"\n",
    "                    # target_image = self.preprocess(PIL.Image.open(target_image_path))\n",
    "                    \n",
    "                    target_image = self.preprocess(images = PIL.Image.open(target_image_path), return_tensors='pt', padding=True)\n",
    "                    target_image['pixel_values'] = target_image['pixel_values'].squeeze(0)\n",
    "                    return reference_image, target_image, image_captions\n",
    "\n",
    "                elif self.split == 'val':\n",
    "                    target_name = self.triplets[index]['target']\n",
    "                    return reference_name, target_name, image_captions\n",
    "\n",
    "                elif self.split == 'test':\n",
    "                    reference_image_path = base_path / 'fashionIQ_dataset' / 'images' / f\"{reference_name}.jpg\"\n",
    "                    # reference_image = self.preprocess(PIL.Image.open(reference_image_path))\n",
    "                    \n",
    "                    reference_image = self.preprocess(images = PIL.Image.open(reference_image_path), return_tensors='pt', padding=True)\n",
    "                    reference_image['pixel_values'] = reference_image['pixel_values'].squeeze(0)\n",
    "                    return reference_name, reference_image, image_captions\n",
    "\n",
    "            elif self.mode == 'classic':\n",
    "                image_name = self.image_names[index]\n",
    "                image_path = base_path / 'fashionIQ_dataset' / 'images' / f\"{image_name}.jpg\"\n",
    "                # image = self.preprocess(PIL.Image.open(image_path))\n",
    "                \n",
    "                image = self.preprocess(images = PIL.Image.open(image_path), return_tensors='pt', padding=True)\n",
    "                image['pixel_values'] = image['pixel_values'].squeeze(0)\n",
    "                return image_name, image\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"mode should be in ['relative', 'classic']\")\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'relative':\n",
    "            return len(self.triplets)\n",
    "        elif self.mode == 'classic':\n",
    "            return len(self.image_names)\n",
    "        else:\n",
    "            raise ValueError(\"mode should be in ['relative', 'classic']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0853fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fa_clip] *",
   "language": "python",
   "name": "conda-env-fa_clip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
